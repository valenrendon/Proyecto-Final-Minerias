{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVk8tn1pZgxS",
        "outputId": "de98cf0c-d56e-4b9f-9c52-c0ad0bd46e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'scikit-': Expected end or semicolon (after name and no valid version specifier)\n",
            "    scikit-\n",
            "          ^\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: scikit-optimize in /usr/local/lib/python3.12/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6Qh0fsuuRstU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    cross_validate,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    roc_auc_score\n",
        ")\n",
        "# Estadísticas\n",
        "from scipy import stats\n",
        "from scipy.stats import f_oneway\n",
        "import itertools\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaacXnX3EQ72"
      },
      "source": [
        "##CARGAR DATOS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xN-k7SPcq4A8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQIEn-9AEbJZ",
        "outputId": "90b87e9c-bda6-42b4-a68e-077011d6150b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Conjunto de entrenamiento: 409390 muestras, 17 características\n",
            "Conjunto de prueba: 95939 muestras\n",
            "\n",
            "Distribución de clases en entrenamiento:\n",
            "HeartDisease\n",
            "No     204695\n",
            "Yes    204695\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv(\"heart_disease_train.csv\")\n",
        "X_train = df_train.drop('HeartDisease', axis=1)\n",
        "y_train = df_train['HeartDisease'].map({\"No\": 0, \"Yes\": 1})\n",
        "\n",
        "df_test = pd.read_csv(\"heart_disease_test.csv\")\n",
        "X_test = df_test.drop('HeartDisease', axis=1)\n",
        "y_test = df_test['HeartDisease'].map({\"No\": 0, \"Yes\": 1})\n",
        "\n",
        "print(f\" Conjunto de entrenamiento: {X_train.shape[0]} muestras, {X_train.shape[1]} características\")\n",
        "print(f\"Conjunto de prueba: {X_test.shape[0]} muestras\")\n",
        "print(f\"\\nDistribución de clases en entrenamiento:\")\n",
        "print(df_train['HeartDisease'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Definir columnas y preprocesador\n"
      ],
      "metadata": {
        "id": "-AWDeBEmsDWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = ['BMI', 'PhysicalHealth', 'SleepTime', 'MentalHealth']\n",
        "\n",
        "categorical_cols = [\n",
        "    'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex',\n",
        "    'AgeCategory', 'Race', 'Diabetic', 'PhysicalActivity',\n",
        "    'GenHealth', 'Asthma', 'KidneyDisease', 'SkinCancer'\n",
        "]\n",
        "\n",
        "# Cambio clave: sparse_output=False → genera matriz densa (necesario para GaussianNB)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n"
      ],
      "metadata": {
        "id": "zF8EcSAsq3Mt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nTatrdkEWWH"
      },
      "source": [
        "##DEFINIR MODELOS BASE (SIN GRID SEARCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENzGX8q2R3C-",
        "outputId": "425df274-2806-4d38-ba6a-9cd8ef9e4af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos listos para entrenamiento con pipeline completo:\n",
            "Decision Tree\n",
            "Regresión Logística\n",
            "Naive Bayes\n",
            "Random Forest\n",
            "Gradient Boosting\n",
            "AdaBoost\n",
            "XGBoost\n"
          ]
        }
      ],
      "source": [
        "# Modelos sin optimización, con preprocesamiento incluido\n",
        "base_models = {\n",
        "    'Decision Tree': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', DecisionTreeClassifier(\n",
        "            criterion='gini',\n",
        "            max_depth=5,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=5,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "\n",
        "    'Regresión Logística': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "\n",
        "    'Naive Bayes': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', GaussianNB())\n",
        "    ]),\n",
        "\n",
        "    'Random Forest': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=5,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ]),\n",
        "\n",
        "    'Gradient Boosting': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=5,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=5,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "\n",
        "    'AdaBoost': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', AdaBoostClassifier(\n",
        "            estimator=DecisionTreeClassifier(max_depth=3),\n",
        "            n_estimators=50,\n",
        "            learning_rate=1.0,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "\n",
        "    'XGBoost': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='logloss'\n",
        "        ))\n",
        "    ])\n",
        "}\n",
        "\n",
        "print(\"Modelos listos para entrenamiento con pipeline completo:\")\n",
        "for name in base_models.keys():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPDTVGCEb0M"
      },
      "source": [
        "##VALIDACIÓN CRUZADA - MODELOS BASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEl4pa_3Ebel",
        "outputId": "e8ef432f-69a1-4e3c-a33b-4df176d13166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando modelos base...\n",
            "\n",
            "Evaluando Decision Tree...\n",
            "  F1: 0.7294 | Recall: 0.7601 | Precision: 0.7011 | Accuracy: 0.7180\n",
            "Evaluando Regresión Logística...\n",
            "  F1: 0.7694 | Recall: 0.7872 | Precision: 0.7524 | Accuracy: 0.7640\n",
            "Evaluando Naive Bayes...\n",
            "  F1: 0.7492 | Recall: 0.8973 | Precision: 0.6430 | Accuracy: 0.6996\n",
            "Evaluando Random Forest...\n",
            "  F1: 0.7659 | Recall: 0.7679 | Precision: 0.7638 | Accuracy: 0.7653\n",
            "Evaluando Gradient Boosting...\n",
            "  F1: 0.8333 | Recall: 0.8312 | Precision: 0.8355 | Accuracy: 0.8338\n",
            "Evaluando AdaBoost...\n",
            "  F1: 0.8159 | Recall: 0.8058 | Precision: 0.8264 | Accuracy: 0.8182\n",
            "Evaluando XGBoost...\n",
            "  F1: 0.8334 | Recall: 0.8350 | Precision: 0.8318 | Accuracy: 0.8331\n"
          ]
        }
      ],
      "source": [
        "cv_folds = 3\n",
        "kfold = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "cv_results_list = []\n",
        "\n",
        "print(\"\\nEvaluando modelos base...\\n\")\n",
        "\n",
        "for model_name, model in base_models.items():\n",
        "    print(f\"Evaluando {model_name}...\")\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train, y_train,\n",
        "        cv=kfold,\n",
        "        scoring=scoring,\n",
        "        n_jobs=-1,\n",
        "        return_train_score=False,\n",
        "        error_score='raise'\n",
        "    )\n",
        "\n",
        "    cv_results_list.append({\n",
        "        'Modelo': model_name,\n",
        "        'Accuracy_CV': scores['test_accuracy'].mean(),\n",
        "        'Precision_CV': scores['test_precision'].mean(),\n",
        "        'Recall_CV': scores['test_recall'].mean(),\n",
        "        'F1_CV': scores['test_f1'].mean(),\n",
        "        'ROC_AUC_CV': scores['test_roc_auc'].mean()\n",
        "    })\n",
        "\n",
        "    print(f\"  F1: {scores['test_f1'].mean():.4f} | Recall: {scores['test_recall'].mean():.4f} | Precision: {scores['test_precision'].mean():.4f} | Accuracy: {scores['test_accuracy'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJDlBPXSHaq"
      },
      "source": [
        "##Anova y Toukey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nAVaj3kZ7gX",
        "outputId": "7714976f-326f-485a-fd77-1bfee0d3a008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluando Decision Tree...\n",
            "  Evaluando Regresión Logística...\n",
            "  Evaluando Naive Bayes...\n",
            "  Evaluando Random Forest...\n",
            "  Evaluando Gradient Boosting...\n",
            "  Evaluando AdaBoost...\n",
            "  Evaluando XGBoost...\n",
            "ESTADÍSTICAS DESCRIPTIVAS (F1-Score por modelo):\n",
            "----------------------------------------------------------------------------------------------------\n",
            "             Modelo  Media    Std    Min    Max\n",
            "            XGBoost 0.8334 0.0004 0.8330 0.8340\n",
            "  Gradient Boosting 0.8333 0.0011 0.8319 0.8345\n",
            "           AdaBoost 0.8159 0.0025 0.8128 0.8189\n",
            "Regresión Logística 0.7694 0.0006 0.7688 0.7701\n",
            "      Random Forest 0.7659 0.0013 0.7642 0.7675\n",
            "        Naive Bayes 0.7492 0.0004 0.7489 0.7498\n",
            "      Decision Tree 0.7294 0.0013 0.7276 0.7305\n",
            "\n",
            "Estadístico F: 2161.2385\n",
            "P-value: 0.000000\n",
            "\n",
            " Resultado: Hay diferencias SIGNIFICATIVAS entre los modelos (p < 0.05)\n",
            "  → Procederemos con el Test de Tukey para comparaciones múltiples\n",
            "TEST DE TUKEY (Honestly Significant Difference)\n",
            "\n",
            " Advertencia: scikit-posthocs no está instalado\n",
            "Instalando: pip install scikit-posthocs\n",
            "\n",
            "Realizando comparaciones pareadas manualmente con t-test...\n",
            "\n",
            "Matriz de p-values (t-test pareado):\n",
            "(Valores < 0.05 indican diferencias significativas)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "                     Decision Tree  Regresión Logística  Naive Bayes  Random Forest  Gradient Boosting  AdaBoost  XGBoost\n",
            "Decision Tree               1.0000               0.0000       0.0000         0.0000             0.0000    0.0000   0.0000\n",
            "Regresión Logística         0.0000               1.0000       0.0000         0.0275             0.0000    0.0000   0.0000\n",
            "Naive Bayes                 0.0000               0.0000       1.0000         0.0001             0.0000    0.0000   0.0000\n",
            "Random Forest               0.0000               0.0275       0.0001         1.0000             0.0000    0.0000   0.0000\n",
            "Gradient Boosting           0.0000               0.0000       0.0000         0.0000             1.0000    0.0008   0.9241\n",
            "AdaBoost                    0.0000               0.0000       0.0000         0.0000             0.0008    1.0000   0.0006\n",
            "XGBoost                     0.0000               0.0000       0.0000         0.0000             0.9241    0.0006   1.0000\n",
            "SELECCIÓN DE LOS 3 MEJORES MODELOS\n",
            "\n",
            " TOP 3 MODELOS SELECCIONADOS (basado en F1-Score medio):\n",
            "1. XGBoost                   F1-Score: 0.8334 (±0.0004)\n",
            "2. Gradient Boosting         F1-Score: 0.8333 (±0.0011)\n",
            "3. AdaBoost                  F1-Score: 0.8159 (±0.0025)\n",
            "\n",
            " ANÁLISIS DE DIFERENCIAS ENTRE TOP 3:\n",
            "  • XGBoost vs Gradient Boosting: p=0.9241 → Sin diferencia significativa\n",
            "  • XGBoost vs AdaBoost: p=0.0006 → Diferencia SIGNIFICATIVA\n",
            "  • Gradient Boosting vs AdaBoost: p=0.0008 → Diferencia SIGNIFICATIVA\n",
            "ESTOS 3 MODELOS PASARÁN AL PROCESO DE OPTIMIZACIÓN (GRID SEARCH)\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Recolectar todos los scores de F1 por fold para cada modelo\n",
        "cv_folds = 3\n",
        "kfold = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Diccionario para almacenar scores por modelo\n",
        "scores_by_model = {}\n",
        "\n",
        "for model_name, model in base_models.items():\n",
        "    print(f\"  Evaluando {model_name}...\")\n",
        "\n",
        "    #Usar diccionario en scoring\n",
        "    scores = cross_validate(\n",
        "        model, X_train, y_train,\n",
        "        cv=kfold,\n",
        "        scoring={'f1': 'f1'},\n",
        "        n_jobs=-1,\n",
        "        return_train_score=False\n",
        "    )\n",
        "\n",
        "    scores_by_model[model_name] = scores['test_f1']\n",
        "\n",
        "# Crear DataFrame para análisis\n",
        "\n",
        "# Mostrar estadísticas descriptivas\n",
        "print(\"ESTADÍSTICAS DESCRIPTIVAS (F1-Score por modelo):\")\n",
        "print(\"-\" * 100)\n",
        "stats_df = pd.DataFrame({\n",
        "    'Modelo': list(scores_by_model.keys()),\n",
        "    'Media': [np.mean(scores) for scores in scores_by_model.values()],\n",
        "    'Std': [np.std(scores) for scores in scores_by_model.values()],\n",
        "    'Min': [np.min(scores) for scores in scores_by_model.values()],\n",
        "    'Max': [np.max(scores) for scores in scores_by_model.values()]\n",
        "})\n",
        "stats_df = stats_df.sort_values('Media', ascending=False)\n",
        "print(stats_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# TEST ANOVA\n",
        "\n",
        "# Preparar datos para ANOVA\n",
        "model_scores = list(scores_by_model.values())\n",
        "\n",
        "# Realizar ANOVA\n",
        "f_statistic, p_value = f_oneway(*model_scores)\n",
        "\n",
        "print(f\"\\nEstadístico F: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.6f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"\\n Resultado: Hay diferencias SIGNIFICATIVAS entre los modelos (p < 0.05)\")\n",
        "    print(\"  → Procederemos con el Test de Tukey para comparaciones múltiples\")\n",
        "else:\n",
        "    print(\"\\n Resultado: NO hay diferencias significativas entre los modelos (p >= 0.05)\")\n",
        "    print(\"  Los modelos tienen rendimientos estadísticamente similares\")\n",
        "\n",
        "# TEST DE TUKEY (Post-hoc)\n",
        "print(\"TEST DE TUKEY (Honestly Significant Difference)\")\n",
        "\n",
        "\n",
        "# Crear DataFrame en formato largo para Tukey\n",
        "tukey_data = []\n",
        "for model_name, scores in scores_by_model.items():\n",
        "    for score in scores:\n",
        "        tukey_data.append({'Modelo': model_name, 'F1_Score': score})\n",
        "\n",
        "tukey_df = pd.DataFrame(tukey_data)\n",
        "\n",
        "# Instalar scikit-posthocs si no está instalado\n",
        "try:\n",
        "    from scikit_posthocs import posthoc_tukey\n",
        "\n",
        "    # Realizar Test de Tukey\n",
        "    tukey_result = posthoc_tukey(tukey_df, val_col='F1_Score', group_col='Modelo')\n",
        "\n",
        "    print(\"\\nMatriz de p-values del Test de Tukey:\")\n",
        "    print(\"(Valores < 0.05 indican diferencias significativas entre modelos)\")\n",
        "    print(\"-\" * 100)\n",
        "    print(tukey_result.to_string(float_format='%.4f'))\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\n Advertencia: scikit-posthocs no está instalado\")\n",
        "    print(\"Instalando: pip install scikit-posthocs\")\n",
        "    print(\"\\nRealizando comparaciones pareadas manualmente con t-test...\")\n",
        "\n",
        "    # Alternativa: t-test pareado manual\n",
        "    from scipy.stats import ttest_ind\n",
        "\n",
        "    model_names = list(scores_by_model.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Crear matriz de p-values\n",
        "    p_matrix = np.ones((n_models, n_models))\n",
        "\n",
        "    for i, model1 in enumerate(model_names):\n",
        "        for j, model2 in enumerate(model_names):\n",
        "            if i != j:\n",
        "                t_stat, p_val = ttest_ind(scores_by_model[model1],\n",
        "                                         scores_by_model[model2])\n",
        "                p_matrix[i, j] = p_val\n",
        "\n",
        "    tukey_result = pd.DataFrame(p_matrix,\n",
        "                               index=model_names,\n",
        "                               columns=model_names)\n",
        "\n",
        "    print(\"\\nMatriz de p-values (t-test pareado):\")\n",
        "    print(\"(Valores < 0.05 indican diferencias significativas)\")\n",
        "    print(\"-\" * 100)\n",
        "    print(tukey_result.to_string(float_format='%.4f'))\n",
        "\n",
        "# SELECCIÓN DE LOS 3 MEJORES MODELOS\n",
        "\n",
        "print(\"SELECCIÓN DE LOS 3 MEJORES MODELOS\")\n",
        "\n",
        "# Ordenar modelos por media de F1-Score\n",
        "top_3_models = stats_df.head(3)['Modelo'].tolist()\n",
        "\n",
        "print(\"\\n TOP 3 MODELOS SELECCIONADOS (basado en F1-Score medio):\")\n",
        "for i, model_name in enumerate(top_3_models, 1):\n",
        "    mean_f1 = stats_df[stats_df['Modelo'] == model_name]['Media'].values[0]\n",
        "    std_f1 = stats_df[stats_df['Modelo'] == model_name]['Std'].values[0]\n",
        "    print(f\"{i}. {model_name:<25} F1-Score: {mean_f1:.4f} (±{std_f1:.4f})\")\n",
        "\n",
        "# Verificar si hay diferencias significativas entre el top 3\n",
        "print(\"\\n ANÁLISIS DE DIFERENCIAS ENTRE TOP 3:\")\n",
        "\n",
        "for i in range(len(top_3_models)):\n",
        "    for j in range(i+1, len(top_3_models)):\n",
        "        model1 = top_3_models[i]\n",
        "        model2 = top_3_models[j]\n",
        "        p_val = tukey_result.loc[model1, model2]\n",
        "\n",
        "        if p_val < 0.05:\n",
        "            print(f\"  • {model1} vs {model2}: p={p_val:.4f} → Diferencia SIGNIFICATIVA\")\n",
        "        else:\n",
        "            print(f\"  • {model1} vs {model2}: p={p_val:.4f} → Sin diferencia significativa\")\n",
        "\n",
        "\n",
        "print(\"ESTOS 3 MODELOS PASARÁN AL PROCESO DE OPTIMIZACIÓN (GRID SEARCH)\")\n",
        "\n",
        "# Guardar los nombres de los top 3 para usar después\n",
        "selected_models = {name: base_models[name] for name in top_3_models}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CYnnVaCEhF2"
      },
      "source": [
        "##OPTIMIZACIÓN DE LOS 3 MEJORES MODELOS (GRID SEARCH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "param_grids = {\n",
        "    'XGBoost': {\n",
        "        'classifier__n_estimators': [100],\n",
        "        'classifier__max_depth': [4, 6],\n",
        "        'classifier__learning_rate': [0.1],\n",
        "        'classifier__subsample': [0.8]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'classifier__n_estimators': [100],\n",
        "        'classifier__learning_rate': [0.1],\n",
        "        'classifier__max_depth': [3]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'classifier__n_estimators': [50],\n",
        "        'classifier__learning_rate': [1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for model_name, model in selected_models.items():\n",
        "    print(f\" Optimizando {model_name}...\")\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grids[model_name],\n",
        "        scoring='f1',\n",
        "        cv=2,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "    print(f\" Mejor F1-score (CV): {grid_search.best_score_:.4f}\")\n",
        "    print(f\" Mejores hiperparámetros: {grid_search.best_params_}\")\n",
        "    print(f\" Tiempo total: {elapsed/60:.2f} minutos\")\n",
        "\n",
        "print(\"Optimización express completada.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvYK15qJGKQB",
        "outputId": "a97f8b63-7f0c-4dcb-cb14-79e6a72c3ab1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Optimizando XGBoost...\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            " Mejor F1-score (CV): 0.8274\n",
            " Mejores hiperparámetros: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 6, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
            " Tiempo total: 0.55 minutos\n",
            " Optimizando Gradient Boosting...\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            " Mejor F1-score (CV): 0.8067\n",
            " Mejores hiperparámetros: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
            " Tiempo total: 4.56 minutos\n",
            " Optimizando AdaBoost...\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            " Mejor F1-score (CV): 0.8079\n",
            " Mejores hiperparámetros: {'classifier__learning_rate': 1.0, 'classifier__n_estimators': 50}\n",
            " Tiempo total: 2.60 minutos\n",
            "Optimización express completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Usar 10% del dataset de entrenamiento para optimización rápida\n",
        "X_small = X_train.sample(frac=0.1, random_state=42)\n",
        "y_small = y_train.loc[X_small.index]\n",
        "print(f\"Usando muestra de {len(X_small)} registros para optimización rápida.\\n\")\n",
        "\n",
        "# Parámetros de control\n",
        "N_ITER_RS = 5   # número de combinaciones aleatorias\n",
        "CV = 2          # número de folds\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# -----------------------------\n",
        "# Definir los estimadores base (sin pipeline interno)\n",
        "# -----------------------------\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "estimators_base = {\n",
        "    'XGBoost': XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss'\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    'AdaBoost': AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=3),\n",
        "        n_estimators=50,\n",
        "        learning_rate=1.0,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Espacios de búsqueda de hiperparámetros\n",
        "# -----------------------------\n",
        "param_spaces = {\n",
        "    'XGBoost': {\n",
        "        'clf__n_estimators': stats.randint(50, 150),\n",
        "        'clf__max_depth': stats.randint(3, 8),\n",
        "        'clf__learning_rate': stats.uniform(0.01, 0.2),\n",
        "        'clf__subsample': stats.uniform(0.7, 0.3),\n",
        "        'clf__colsample_bytree': stats.uniform(0.7, 0.3),\n",
        "        'clf__min_child_weight': stats.randint(1, 4)\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'clf__n_estimators': stats.randint(50, 150),\n",
        "        'clf__learning_rate': stats.uniform(0.01, 0.2),\n",
        "        'clf__max_depth': stats.randint(3, 6),\n",
        "        'clf__subsample': stats.uniform(0.7, 0.3)\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'clf__n_estimators': stats.randint(25, 100),\n",
        "        'clf__learning_rate': stats.uniform(0.01, 1.0)\n",
        "    }\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Optimización de los top 3 modelos\n",
        "# -----------------------------\n",
        "optimized_models = {}\n",
        "\n",
        "for model_name in top_3_models:  # lista de los top 3 modelos\n",
        "    print(f\"\\nOPTIMIZANDO: {model_name}\")\n",
        "\n",
        "    base_estimator = estimators_base[model_name]\n",
        "\n",
        "    # Crear pipeline: preprocesamiento + modelo\n",
        "    pipe = Pipeline([\n",
        "        ('preproc', preprocessor),\n",
        "        ('clf', base_estimator)\n",
        "    ])\n",
        "\n",
        "    # RandomizedSearchCV\n",
        "    rs = RandomizedSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_distributions=param_spaces[model_name],\n",
        "        n_iter=N_ITER_RS,\n",
        "        scoring='f1',\n",
        "        cv=CV,\n",
        "        random_state=RANDOM_STATE,\n",
        "        refit=True,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    t0 = time.time()\n",
        "    rs.fit(X_small, y_small)\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(f\"{model_name} optimizado en {(t1 - t0)/60:.2f} minutos\")\n",
        "    print(f\"   Mejor F1 (CV): {rs.best_score_:.4f}\")\n",
        "    print(f\"   Mejores parámetros: {rs.best_params_}\")\n",
        "\n",
        "    optimized_models[model_name] = rs.best_estimator_\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluación rápida en la muestra\n",
        "# -----------------------------\n",
        "print(\"\\nEvaluando desempeño en la muestra de entrenamiento reducida:\")\n",
        "scores_summary = []\n",
        "for name, model in optimized_models.items():\n",
        "    y_proba = model.predict_proba(X_small)[:, 1]\n",
        "    auc = roc_auc_score(y_small, y_proba)\n",
        "    scores_summary.append({'Modelo': name, 'ROC_AUC': auc})\n",
        "    print(f\"{name}: ROC_AUC = {auc:.4f}\")\n",
        "\n",
        "scores_df = pd.DataFrame(scores_summary).sort_values('ROC_AUC', ascending=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Selección del mejor modelo\n",
        "# -----------------------------\n",
        "best_model_name = scores_df.iloc[0]['Modelo']\n",
        "final_pipeline = optimized_models[best_model_name]\n",
        "print(f\"\\nMejor modelo final: {best_model_name}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Reentrenamiento con todo el conjunto de entrenamiento\n",
        "# -----------------------------\n",
        "print(\"Reentrenando el mejor modelo con todo el conjunto de entrenamiento...\")\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "print(\"Reentrenamiento completo.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Guardar pipeline final\n",
        "# -----------------------------\n",
        "joblib.dump(final_pipeline, 'mejor_pipeline_opt.joblib')\n",
        "print(f\"Pipeline '{best_model_name}' guardado correctamente como 'mejor_pipeline_opt.joblib'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8JfV7UdUPTE",
        "outputId": "2a914bcc-f484-42c9-b764-effc332d855b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando muestra de 40939 registros para optimización rápida.\n",
            "\n",
            "\n",
            "OPTIMIZANDO: XGBoost\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "XGBoost optimizado en 0.16 minutos\n",
            "   Mejor F1 (CV): 0.8322\n",
            "   Mejores parámetros: {'clf__colsample_bytree': np.float64(0.7139996989640846), 'clf__learning_rate': np.float64(0.20475110376829186), 'clf__max_depth': 5, 'clf__min_child_weight': 3, 'clf__n_estimators': 113, 'clf__subsample': np.float64(0.8400288679743939)}\n",
            "\n",
            "OPTIMIZANDO: Gradient Boosting\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "Gradient Boosting optimizado en 0.95 minutos\n",
            "   Mejor F1 (CV): 0.8364\n",
            "   Mejores parámetros: {'clf__learning_rate': np.float64(0.12973169683940733), 'clf__max_depth': 5, 'clf__n_estimators': 132, 'clf__subsample': np.float64(0.7299924747454009)}\n",
            "\n",
            "OPTIMIZANDO: AdaBoost\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
            "AdaBoost optimizado en 0.71 minutos\n",
            "   Mejor F1 (CV): 0.8090\n",
            "   Mejores parámetros: {'clf__learning_rate': np.float64(0.7419939418114051), 'clf__n_estimators': 85}\n",
            "\n",
            "Evaluando desempeño en la muestra de entrenamiento reducida:\n",
            "XGBoost: ROC_AUC = 0.9369\n",
            "Gradient Boosting: ROC_AUC = 0.9418\n",
            "AdaBoost: ROC_AUC = 0.9126\n",
            "\n",
            "Mejor modelo final: Gradient Boosting\n",
            "Reentrenando el mejor modelo con todo el conjunto de entrenamiento...\n",
            "Reentrenamiento completo.\n",
            "Pipeline 'Gradient Boosting' guardado correctamente como 'mejor_pipeline_opt.joblib'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba de que guardo bien"
      ],
      "metadata": {
        "id": "8vYw486VYfZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar pipeline guardado\n",
        "pipeline = joblib.load('mejor_pipeline_opt.joblib')\n",
        "\n",
        "# Verificar columnas del dataset de prueba (usa X_test o una muestra del train)\n",
        "X_test_sample = X_train.sample(10, random_state=42)  # solo una muestra pequeña\n",
        "y_test_sample = y_train.loc[X_test_sample.index]\n",
        "\n",
        "# Comprobar predict_proba\n",
        "probas = pipeline.predict_proba(X_test_sample)[:, 1]\n",
        "print(\"Probabilidades de la muestra:\", probas)\n",
        "\n",
        "# Revisar que no todas las probabilidades sean iguales\n",
        "if np.all(probas == probas[0]):\n",
        "    print(\" ERROR: Todas las probabilidades son iguales\")\n",
        "else:\n",
        "    print(\"Probabilidades variadas: OK\")\n",
        "\n",
        "# Comprobar predict\n",
        "preds = pipeline.predict(X_test_sample)\n",
        "print(\"Predicciones de la muestra:\", preds)\n",
        "\n",
        "# Validar tipos y cantidad de columnas\n",
        "expected_cols = X_train.columns\n",
        "if not all(col in X_test_sample.columns for col in expected_cols):\n",
        "    print(\"ERROR: Faltan columnas en los datos de entrada\")\n",
        "else:\n",
        "    print(\" Columnas de entrada correctas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-KnXheMYdWh",
        "outputId": "b8b5e575-692f-4a18-a9cc-c434cc92ce3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades de la muestra: [0.12246319 0.06666031 0.39435591 0.99197603 0.98172639 0.43927771\n",
            " 0.12646433 0.29131529 0.06687801 0.64316251]\n",
            "Probabilidades variadas: OK\n",
            "Predicciones de la muestra: [0 0 0 1 1 0 0 0 0 1]\n",
            " Columnas de entrada correctas\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}