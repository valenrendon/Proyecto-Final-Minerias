# -*- coding: utf-8 -*-
"""Limpieza_de_datos_final_Mineria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qdGeZcjfvOaZ-2YMkbp4yXy92Im-TjQJ
"""

!pip install ydata-profiling

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import chi2_contingency
from sklearn.feature_selection import mutual_info_classif
from scipy.stats import f_oneway
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from imblearn.combine import SMOTETomek
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from ydata_profiling import ProfileReport
from imblearn.combine import SMOTETomek
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

"""## Carga de datos"""

#cargamos los datos
df = pd.read_csv("/content/heart_2020_cleaned.csv")

df.head(10)

df.info()

"""## Seleccion de variables"""

#Pasamos las columnas tipo object a category
df['HeartDisease'] = df['HeartDisease'].astype('category')
df['Smoking'] = df['Smoking'].astype('category')
df['AlcoholDrinking'] = df['AlcoholDrinking'].astype('category')
df['Stroke'] = df['Stroke'].astype('category')
df['DiffWalking'] = df['DiffWalking'].astype('category')
df['Sex'] = df['Sex'].astype('category')
df['AgeCategory'] = df['AgeCategory'].astype('category')
df['Race'] = df['Race'].astype('category')
df['Diabetic'] = df['Diabetic'].astype('category')
df['PhysicalActivity'] = df['PhysicalActivity'].astype('category')
df['GenHealth'] = df['GenHealth'].astype('category')
df['Asthma'] = df['Asthma'].astype('category')
df['KidneyDisease'] = df['KidneyDisease'].astype('category')
df['SkinCancer'] = df['SkinCancer'].astype('category')

df.info()

X = df.drop('HeartDisease', axis=1)
y = df['HeartDisease']

"""## Estadistica Descriptiva"""

# Crear el perfil de pandas profiling
profile = ProfileReport(df, title="Pandas Profiling Report - Heart Disease", explorative=True)

# Ver en notebook
profile.to_notebook_iframe()

print("\nForma del dataset:", df.shape)

#Describimos variables numericas
df.describe()

#BoxPlot para las numericas
num_cols = df.select_dtypes(include='float64').columns

for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=col, data=df)
    plt.title(f'Distribución de {col}')
    plt.xlabel(col)
    plt.ylabel('Frecuencia')
    plt.tight_layout()
    plt.show()

#Describimos variables categoricas
df.describe(include='category').T

#Diagrama de barras para las categoricas
cat_cols = df.select_dtypes(include='category').columns

for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, data=df, palette='Set2')
    plt.title(f'Distribución de {col}')
    plt.xticks(rotation=45)
    plt.xlabel(col)
    plt.ylabel('Frecuencia')
    plt.tight_layout()
    plt.show()

#Distribucion de la variable objetivo
df['HeartDisease'].value_counts(normalize=True) * 100

"""## Limpieza de nulos"""

print("\nCantidad de valores nulos por columna:")
print(df.isnull().sum())

df[df.isnull().any(axis=1)].head(10)

df = df.dropna()
df.info()

"""## Analisis de Correlaciones para Redundancia"""

#Redundancia para variables numericas
# Filtramos solo variables numéricas
num_vars = df.select_dtypes(include=['number'])

# Calculamos la matriz de correlación
corr = num_vars.corr(method='pearson')

# Mostramos un mapa de calor
plt.figure(figsize=(10,8))
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title("Correlación entre variables numéricas")
plt.show()

#Redundancia entre variables categoricas
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    r, k = confusion_matrix.shape
    return np.sqrt(chi2 / (n * (min(k, r) - 1)))

# Creamos la matriz de Cramer's V
cat_vars = df.select_dtypes(include='category').columns
cramer_matrix = pd.DataFrame(index=cat_vars, columns=cat_vars)

for var1 in cat_vars:
    for var2 in cat_vars:
        if var1 == var2:
            cramer_matrix.loc[var1, var2] = 1
        else:
            cramer_matrix.loc[var1, var2] = cramers_v(df[var1], df[var2])

cramer_matrix = cramer_matrix.astype(float)

plt.figure(figsize=(10,8))
sns.heatmap(cramer_matrix, annot=True, cmap='Purples')
plt.title("Asociación (Cramer's V) entre variables categóricas")
plt.show()

df.hist()

"""division 70-30 , se balancea solo el 70%"""

from sklearn.model_selection import train_test_split

# Separar variables predictoras (X) y variable objetivo (y)
X = df.drop('HeartDisease', axis=1)
y = df['HeartDisease']

# División 70-30 (entrenamiento-prueba)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,          # 30% de los datos para prueba
    random_state=42,        # Semilla para reproducibilidad
    stratify=y              # Mantiene la proporción de clases
)

print("Tamaño del conjunto de entrenamiento:", X_train.shape)
print("Tamaño del conjunto de prueba:", X_test.shape)
print("Distribución en entrenamiento:\n", y_train.value_counts(normalize=True))
print("Distribución en prueba:\n", y_test.value_counts(normalize=True))

"""##Balanceo"""

# Identificar columnas categóricas y numéricas
categorical_features = X_train.select_dtypes(include='category').columns
numerical_features = X_train.select_dtypes(include=np.number).columns

# Crear preprocesador
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('num', 'passthrough', numerical_features)
    ],
    remainder='drop'
)

# Aplicar preprocesamiento al conjunto de entrenamiento
X_train_processed = preprocessor.fit_transform(X_train)

# Obtener nombres de columnas después del OneHotEncoder
encoded_cols = preprocessor.get_feature_names_out()

# Convertir a DataFrame con nombres correctos
X_train_processed_df = pd.DataFrame(
    X_train_processed.toarray() if hasattr(X_train_processed, "toarray") else X_train_processed,
    columns=encoded_cols
)

# Aplicar SMOTE con balanceo al 100%
smote = SMOTE(sampling_strategy=1.0, random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train_processed_df, y_train)

# Asegurar que y_train_bal sea una Serie de pandas
y_train_bal = pd.Series(y_train_bal, name='HeartDisease')

# Mostrar resultados
print("Distribución original:\n", y_train.value_counts(), "\n")
print("Distribución después del balanceo (100%):\n", y_train_bal.value_counts())
print("\nForma de X_train_bal después del balanceo y codificación:", X_train_bal.shape)

"""##Descargar

Datos balanceados para entrenar el modelo

Datos originales de prueba
"""

# Aplicar el mismo preprocesamiento al conjunto de prueba
X_test_processed = preprocessor.transform(X_test)

# Convertir a DataFrame con las mismas columnas codificadas
X_test_processed_df = pd.DataFrame(
    X_test_processed.toarray() if hasattr(X_test_processed, "toarray") else X_test_processed,
    columns=encoded_cols
)

# Agregar la variable objetivo (y_test)
df_test = pd.DataFrame(X_test_processed_df)
df_test['HeartDisease'] = y_test.values

# Guardar el conjunto de prueba en un archivo CSV
df_test.to_csv('heart_disease_test.csv', index=False)

# Confirmar resultados
print("Archivo 'heart_disease_test.csv' creado correctamente.")
print("Tamaño del conjunto de prueba:", df_test.shape)
print("Distribución de clases en test:\n", df_test['HeartDisease'].value_counts())

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import pandas as pd

# 1. División 70-30 (sin tocar las variables categóricas)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.30,
    random_state=42,
    stratify=y
)

print("Distribución original en entrenamiento:")
print(y_train.value_counts())
print(f"\nForma de X_train antes del balanceo: {X_train.shape}")

# 2. Para SMOTE, necesitamos convertir temporalmente a numérico
# pero luego restauramos las categorías originales
X_train_numeric = X_train.copy()

# Convertir categóricas a códigos numéricos temporalmente
categorical_cols = X_train.select_dtypes(include='category').columns
category_mappings = {}

for col in categorical_cols:
    category_mappings[col] = X_train[col].cat.categories
    X_train_numeric[col] = X_train[col].cat.codes

# 3. Aplicar SMOTE
smote = SMOTE(sampling_strategy=1.0, random_state=42)
X_train_bal_numeric, y_train_bal = smote.fit_resample(X_train_numeric, y_train)

# 4. Restaurar las variables categóricas originales
X_train_bal = X_train_bal_numeric.copy()

for col in categorical_cols:
    # Convertir los códigos de vuelta a categorías
    X_train_bal[col] = pd.Categorical.from_codes(
        X_train_bal_numeric[col].astype(int),
        categories=category_mappings[col]
    )

# Asegurar que y_train_bal sea una Serie
y_train_bal = pd.Series(y_train_bal, name='HeartDisease')

# 5. Mostrar resultados
print("\n" + "="*50)
print("Distribución después del balanceo:")
print(y_train_bal.value_counts())
print(f"\nForma de X_train_bal: {X_train_bal.shape}")
print(f"\nTipos de datos en X_train_bal:")
print(X_train_bal.dtypes)
print(f"\n¿Las columnas categóricas siguen siendo 'category'? {X_train_bal.select_dtypes(include='category').columns.tolist()}")

# Verificar algunas filas
print("\nPrimeras filas de X_train_bal:")
print(X_train_bal.head())

import pandas as pd

# Crear archivo combinado de entrenamiento
train_bal_combined = X_train_bal.copy()
train_bal_combined['HeartDisease'] = y_train_bal
train_bal_combined.to_csv('heart_disease_train.csv', index=False)
print("✓ heart_disease_train.csv descargado")

# Crear archivo combinado de prueba
test_combined = X_test.copy()
test_combined['HeartDisease'] = y_test
test_combined.to_csv('heart_disease_test.csv', index=False)
print("✓ heart_disease_test.csv descargado")

print("\n" + "="*50)
print("RESUMEN DE ARCHIVOS DESCARGADOS:")
print("="*50)
print(f"1. heart_disease_train.csv - {train_bal_combined.shape[0]} filas × {train_bal_combined.shape[1]} columnas")
print(f"   - Balanceado al 100% con SMOTE")
print(f"   - Variables categóricas originales (sin dummies)")
print(f"\n2. heart_disease_test.csv - {test_combined.shape[0]} filas × {test_combined.shape[1]} columnas")
print(f"   - Sin balancear (distribución original)")
print(f"   - Variables categóricas originales (sin dummies)")

# Mostrar distribución de clases
print("\n" + "="*50)
print("DISTRIBUCIÓN DE CLASES:")
print("="*50)
print("Train (balanceado):")
print(train_bal_combined['HeartDisease'].value_counts())
print("\nTest (original):")
print(test_combined['HeartDisease'].value_counts())

"""##"""